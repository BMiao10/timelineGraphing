{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import os.path\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import plotly.express as px\n",
    "import streamlit as st\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input investigator of interest\n",
    "author = input()\n",
    "\n",
    "# get all the publication pubmed IDs for an author from pubmed\n",
    "id_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?'\n",
    "database = 'pubmed'\n",
    "\n",
    "id_response = requests.post(id_url+'db='+database+'&term='+author+'&retmax=300&retmode=json')\n",
    "id_response = id_response.json()['esearchresult']['idlist']\n",
    "\n",
    "# create string with all id numbers\n",
    "id_num = ','.join(id_response)\n",
    "\n",
    "# new API address for pulling pubmed metadata from IDs \n",
    "metadata_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?'\n",
    "\n",
    "# get all pubmed summary data\n",
    "metadata_response = requests.post(metadata_url + 'db='+database+'&id='+id_num+'&retmode=json')\n",
    "metadata_response = metadata_response.json()['result']\n",
    "\n",
    "# new API address for pulling mesh data from IDs \n",
    "mesh_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?'\n",
    "\n",
    "# get mesh keyword data\n",
    "mesh_response = requests.post(mesh_url + 'db='+database+'&id='+id_num+\"&retmode=xml\")\n",
    "mesh_response = mesh_response.text.split('<PubmedArticle>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize pubmed metadata into dataframe\n",
    "frames = pd.DataFrame()\n",
    "\n",
    "for id_num in id_response:\n",
    "    if id_num in metadata_response: \n",
    "        curr_metadata = metadata_response[id_num]\n",
    "        curr_metadata_list = [id_num, curr_metadata['sortpubdate'], curr_metadata['title']]\n",
    "        curr_metadata_df = pd.DataFrame(np.array(curr_metadata_list))\n",
    "        frames = frames.append(curr_metadata_df.T)\n",
    "\n",
    "frames.columns = ['id', 'Date', 'Title']\n",
    "frames = frames.set_index('id', drop=True)\n",
    "\n",
    "# sort by date\n",
    "frames['Date'] = pd.to_datetime(frames['Date'], errors='ignore')\n",
    "frames = frames.sort_values(by='Date', ascending=True)\n",
    "\n",
    "# create dictionaries for mesh/keyword/doi extractions\n",
    "mesh_dict = {}\n",
    "all_mesh_dict = {}\n",
    "doi_dict = {}\n",
    "\n",
    "# create lists for cleaning up mesh keywords\n",
    "singular_list = []\n",
    "\n",
    "# organize mesh terms into dataframe\n",
    "for mesh_data in mesh_response:\n",
    "    article_id = mesh_data.split('</PMID>')[0].split('<PMID Version=\"1\">')[-1]\n",
    "    doi_id = 'not available'\n",
    "    \n",
    "    if 'ArticleId IdType=\"doi\"' in mesh_data:\n",
    "        doi_id = mesh_data.split('<ArticleId IdType=\"doi\">')[-1].split('</ArticleId>')[0]\n",
    "    \n",
    "    keywords_list = []\n",
    "    \n",
    "    if 'MeshHeadingList' in mesh_data:\n",
    "        keywords_list = mesh_data.split('</MeshHeadingList>')[0].split('<MeshHeadingList>')[-1].split('<MeshHeading>')\n",
    "        keywords_list = [item.split('</DescriptorName>')[0].split('\">')[-1] for item in keywords_list if '</QualifierName>' in item] \n",
    "    \n",
    "    elif 'KeywordList' in mesh_data:\n",
    "        keywords_list = mesh_data.split('</KeywordList>')[0].split('<KeywordList ')[-1].split('\\n')\n",
    "        keywords_list = [item.split('</Keyword>')[0].split('\">')[-1] for item in keywords_list if '</Keyword>' in item]\n",
    "        keywords_list = [item.title() for item in keywords_list]\n",
    "\n",
    "    keywords_list = [item.strip() for item in keywords_list]\n",
    "    keywords_list = [item.replace(' ', '-') for item in keywords_list]\n",
    "    keywords_list = [item.replace('--', '-') for item in keywords_list]\n",
    "    keywords_list = [item.replace(', ', ',') for item in keywords_list]\n",
    "    keywords_list = [item.split(',')[-1]+'-'+item.split(',')[0] if ',' in item else item for item in keywords_list]\n",
    "                \n",
    "    keywords_list = list(set(keywords_list))\n",
    "    \n",
    "    for keyword in keywords_list:\n",
    "        if keyword[-1] != 's': \n",
    "            if keyword not in singular_list: singular_list.append(keyword) \n",
    "\n",
    "    if 'xml' not in article_id: \n",
    "        mesh_dict[article_id] = ' '.join(keywords_list)\n",
    "        doi_dict[article_id] = doi_id\n",
    "\n",
    "mesh_df = pd.DataFrame.from_dict(mesh_dict, orient='index')\n",
    "mesh_df['doi'] = pd.DataFrame.from_dict(doi_dict, orient='index')[0]\n",
    "\n",
    "# merge metadata and mesh data\n",
    "frames = frames.merge(mesh_df, left_index=True, right_index=True, how='outer')\n",
    "frames.columns = ['Date', 'Title','mesh', 'doi']\n",
    "frames = frames.dropna()\n",
    "\n",
    "for ind in frames.index:\n",
    "    if len(frames['mesh'][ind]) < 3:\n",
    "        curr_title = frames['Title'][ind].split(' ')\n",
    "        curr_sortTitle = [word for word in curr_title if word not in text.ENGLISH_STOP_WORDS]\n",
    "        frames['mesh'][ind] = ' '.join(curr_sortTitle)\n",
    "        \n",
    "frames = frames[~(frames['mesh'] == '')]\n",
    "frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generateCategories(frames, max_df, min_df, ngram_max=4, true_k = 5):\n",
    "    \n",
    "    common_words = ['Animal', 'Animals','Adult','Children','Mouse','Mice', 'Child', 'Disease', 'Diseases',\n",
    "                    'Human','Humans','Male','Female','Cell', 'Cells', 'Gene', 'Genes','Protein', 'Proteins', \n",
    "                   'Receptor','Receptors', 'DNA','RNA']\n",
    "\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(common_words)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words=my_stop_words,max_df = max_df,min_df=min_df,ngram_range=(1,ngram_max), \n",
    "                                 lowercase=False,token_pattern=\"(?u)(\\\\b[\\\\w-]+\\\\b)\") \n",
    "    \n",
    "    X = vectorizer.fit_transform(frames['mesh'].to_list())\n",
    "\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=5, algorithm='full')\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    feature_dict = {}\n",
    "    \n",
    "    for i in range(true_k):\n",
    "        for ind in order_centroids[i, :1]: \n",
    "            feature_dict[i] = terms[ind]\n",
    "    \n",
    "    frames['label'] = [model.predict(vectorizer.transform([item])) for item in frames['mesh'] ]\n",
    "    frames['label'] = [feature_dict[item[0]] for item in frames['label'] ]\n",
    "    print(min_df, max_df, model.score(X), feature_dict.values())\n",
    " \n",
    "    return frames\n",
    "\n",
    "frames = generateCategories(frames, 0.99, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "colors = [\"#FF6787\", \"#FFB68C\", \"#FACBC1\", \"#A6DBD7\",\"#63CECE\"]\n",
    "\n",
    "fig = px.scatter(frames, y=\"label\", x=\"Date\", hover_name=\"Title\", color='label',title=author,\n",
    "             color_discrete_sequence=colors, labels = { 'Date': 'Publication Date'})\n",
    "\n",
    "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)','paper_bgcolor': 'rgba(0, 0, 0, 0)',})\n",
    "fig.update_traces(marker_size=10)\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True,\n",
    "                 tickfont={'size':16}, title_font={'size':16})\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, showgrid=True, \n",
    "                 gridwidth=1, gridcolor='LightGrey',tickfont={'size':16}, title_font={'size':1})\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
